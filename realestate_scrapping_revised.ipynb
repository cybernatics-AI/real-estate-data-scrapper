{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1c83a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "\n",
    "def scrape_real_estate(today, yesterday, url):\n",
    "    # Define headers to simulate a browser request\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Referer\": \"https://www.google.com\",\n",
    "        'accept': 'application/json',\n",
    "        'Content-Type': 'application/json',\n",
    "        \"DNT\": \"1\",\n",
    "        \"Jeff\": \"Lango-alas\"\n",
    "    }\n",
    "    \n",
    "    # Request the webpage content\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    property_listings = soup.find_all('div', class_=\"wp-block property list\")\n",
    "\n",
    "    # Initialize dictionary to store data\n",
    "    data = {\n",
    "        \"house_type\": [],\n",
    "        \"house_address\": [],\n",
    "        \"date_listed\": [],\n",
    "        \"price\": [],\n",
    "        \"house_desc\": []\n",
    "    }\n",
    "\n",
    "    # Iterate over each property listing\n",
    "    for listing in property_listings:\n",
    "    \n",
    "        # Extract house type\n",
    "        house_type = listing.find(\"h4\", class_=\"content-title\").text\n",
    "        if \"bedroom\" in house_type:\n",
    "            data[\"house_type\"].append(house_type)        \n",
    "\n",
    "            # Extract house address\n",
    "            raw_address = listing.find(\"address\", class_=\"voffset-bottom-10\").text\n",
    "            normalized_address = unicodedata.normalize(\"NFKD\", raw_address).strip()\n",
    "            data[\"house_address\"].append(normalized_address)\n",
    "\n",
    "            # Extract date listed\n",
    "            date_listed_span = listing.find(\"span\", class_=\"added-on\").text.split()[2:]\n",
    "            recently_added_span = listing.find(\"span\", class_=\"added-on added-recently\")\n",
    "\n",
    "            if not date_listed_span:\n",
    "                if recently_added_span:\n",
    "                    if \"Today\" in recently_added_span.text:\n",
    "                        listing_date = today\n",
    "                    else:\n",
    "                        listing_date = yesterday\n",
    "                    data[\"date_listed\"].append(listing_date)\n",
    "            else:\n",
    "                listing_date = \" \".join(date_listed_span)\n",
    "                data[\"date_listed\"].append(listing_date)\n",
    "\n",
    "            # Extract price\n",
    "            price = listing.select('.price')[1].text\n",
    "            data[\"price\"].append(price)\n",
    "\n",
    "            # Extract house description\n",
    "            description = listing.find(\"ul\", class_=\"aux-info\").text[:-12]\n",
    "            data[\"house_desc\"].append(description)\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d38436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb535fe0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Generate an array of values to check for when to pause execution\n",
    "pause_checkpoints = np.arange(30, 4000, 30)\n",
    "page_number = 2\n",
    "scraped_data = []\n",
    "\n",
    "# Initial URL for scraping\n",
    "base_url = \"https://nigeriapropertycentre.com/for-sale/imo?page=2\"\n",
    "current_url = base_url\n",
    "\n",
    "while page_number <= 19:\n",
    "    # Scrape real estate data\n",
    "    data = scrape_real_estate(\"21 Nov 2022\", \"20 Nov 2022\", url=current_url)\n",
    "    scraped_data.append(data)\n",
    "    \n",
    "    # Check if the current page number is in the pause checkpoints\n",
    "    if page_number in pause_checkpoints:\n",
    "        pause_duration = random.randint(15, 40)\n",
    "        print(f\"Pausing at {current_url} for {pause_duration} seconds.\")\n",
    "        time.sleep(pause_duration)\n",
    "    \n",
    "    # Update URL to the next page\n",
    "    current_url = base_url[:-1] + str(page_number + 1)\n",
    "    page_number += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e20ed96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for item in scraped_data:\n",
    "    df = pd.concat([df, pd.DataFrame(item)], axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
